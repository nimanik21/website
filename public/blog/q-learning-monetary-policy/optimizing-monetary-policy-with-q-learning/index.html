<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Nima Nikopour</title>
<meta name="keywords" content="">
<meta name="description" content="Optimizing Monetary Policy with Q-Learning
Reinforcement learning provides a framework for solving dynamic optimization problems. This article offers a direct example of applying Q-learning to train an agent that conducts monetary policy in a simplified economic environment. This project is broken down into three core coding sections. I begin by defining the EconomyEnv class, which simulates the simplified economy and contains its fundamental rules, including the state variables (inflation, interest rate) and the agent&rsquo;s action space. Next, I build the QLearningAgent class, whose responsibility is to learn the optimal policy by updating its Q-table based on rewards. The final section brings these two together, managing the training process, hyperparameter settings, and the discretization of the continuous state space required for a tabular Q-learning approach.">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/blog/q-learning-monetary-policy/optimizing-monetary-policy-with-q-learning/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css" integrity="sha256-j&#43;ECM6cGvIfy4Is8&#43;XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#004225">
<meta name="msapplication-TileColor" content="#004225">
<link rel="alternate" hreflang="en" href="http://localhost:1313/blog/q-learning-monetary-policy/optimizing-monetary-policy-with-q-learning/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>


    <script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true,
      packages: ['base', 'ams']
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    loader: {
      load: ['[tex]/ams']
    },
    chtml: {
      scale: 1.1
    }
  };
</script>
<script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>


</head>

<body class="" id="top">
    
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Nima Nikopour (Alt + H)">Nima Nikopour</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/cv_2025.pdf" title="CV">
                    <span>CV</span>
                </a>
            </li>
        </ul>
    </nav>
</header>


    <main class="main">
        

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      
    </h1>
    <div class="post-meta">

</div>
  </header> 
  <div class="post-content"><h1 id="optimizing-monetary-policy-with-q-learning">Optimizing Monetary Policy with Q-Learning<a hidden class="anchor" aria-hidden="true" href="#optimizing-monetary-policy-with-q-learning">#</a></h1>
<p>Reinforcement learning provides a framework for solving dynamic optimization problems. This article offers a direct example of applying Q-learning to train an agent that conducts monetary policy in a simplified economic environment. This project is broken down into three core coding sections. I begin by defining the <code>EconomyEnv</code> class, which simulates the simplified economy and contains its fundamental rules, including the state variables (inflation, interest rate) and the agent&rsquo;s action space. Next, I build the <code>QLearningAgent</code> class, whose responsibility is to learn the optimal policy by updating its Q-table based on rewards. The final section brings these two together, managing the training process, hyperparameter settings, and the discretization of the continuous state space required for a tabular Q-learning approach.</p>
<h3 id="the-economic-model-a-simple-world">The Economic Model: A Simple World<a hidden class="anchor" aria-hidden="true" href="#the-economic-model-a-simple-world">#</a></h3>
<p>The agent&rsquo;s universe is governed by a simplified Phillips Curve. This states that raising interest rates tends to lower future inflation, and vice-versa</p>
<div>
$$\pi_{t+1} = \beta \pi_t - \sigma i_t + \epsilon_{t+1} $$
<div>
Here, future inflation $\pi_{t+1}$ is determined by current inflation $\pi_t$, the interest rate set by the agent $i_t$, and a random economic shock $\epsilon_{t+1}$. The agent's only tool to influence this world is the interest rate. Its goal is to keep inflation stable at a target of 2%.
<h3 id="the-learning-problem-states-actions-and-rewards">The Learning Problem: States, Actions, and Rewards<a hidden class="anchor" aria-hidden="true" href="#the-learning-problem-states-actions-and-rewards">#</a></h3>
<p>To teach the agent, I frame the task as a Markov Decision Process:</p>
<ul>
<li><strong>State ($S_t$):</strong> What the agent observes. In this case it&rsquo;s the current inflation rate and interest rate: $(\pi_t, i_t)$.</li>
<li><strong>Action ($A_t$):</strong> The choices the agent can make. I give it five options: a large or standard rate cut, holding steady, or a standard or large rate hike.</li>
<li><strong>Reward ($R_{t+1}$):</strong> A feedback signal that tells the agent how well it&rsquo;s doing. I use a reward shaping technique where the agent is rewarded at every step for making progress towards the 2% inflation target.</li>
</ul>
<h3 id="the-reward-function-specification">The Reward Function Specification<a hidden class="anchor" aria-hidden="true" href="#the-reward-function-specification">#</a></h3>
<p>The objective of the agent is defined by its reward function. To provide continuous feedback and ensure stable learning, I use a dense reward signal composed of three primary components.</p>
<p>For any non-terminal step, the reward $R_{t+1}$ is specified as:</p>
<div>
$$R_{t+1} = \left[ k \cdot (\text{error}_t - \text{error}_{t+1}^{\text{expected}}) \right] - \left[ \alpha \cdot (\Delta i_t)^2 \right]$$
<div>
<p>The components are defined as follows:</p>
<ul>
<li>
<p><strong>Progress Component:</strong> The primary term, <span>$[ k \cdot (\text{error}<em>t - \text{error}</em>{t+1}^{\text{expected}}) ]$<span>, incentivizes the agent to take actions that are expected to reduce the absolute error between inflation and the 2% target. The hyperparameter $k$ scales the magnitude of this incentive.</p>
</li>
<li>
<p><strong>Volatility Penalty:</strong> The term $[ \alpha \cdot (\Delta i_t)^2 ]$ adds a quadratic cost for large changes in the interest rate, promoting a smoother and more stable policy.</p>
</li>
<li>
<p><strong>Terminal Rewards:</strong> An episode concludes with a large terminal reward of <code>+100</code> for achieving the target state or a large penalty of <code>-100</code> if the agent&rsquo;s action leads to economic instability.</p>
</li>
</ul>
<h3 id="deriving-the-reward-function">Deriving the Reward Function<a hidden class="anchor" aria-hidden="true" href="#deriving-the-reward-function">#</a></h3>
<p>The objective of the agent is defined by its reward function. In many reinforcement learning problems, this is a <strong>sparse reward</strong> which is  a single outcome at the very end of a long episode, like a &ldquo;win&rdquo; or &ldquo;loss.&rdquo; This makes it difficult for the agent to know which of its many actions were responsible for the result. To solve this, I use a <strong>dense reward signal</strong>, which provides informative feedback at every single time step.</p>
<p>I employ the technique of Potential-Based Reward Shaping. This method, formalized by Ng, Harada, and Russell (1999), provides a blueprint for adding dense, intermediate rewards while preserving the optimal policy of the original problem. This property, known as policy invariance, ensures that the guidance doesn&rsquo;t inadvertently teach the agent the wrong long-term strategy.</p>
<p>The technique requires defining a &ldquo;potential function,&rdquo; $\phi(s)$, which assigns a scalar value to each state based on its desirability. For this problem, I define the potential as the negative scaled distance from the 2% inflation target:</p>
<p>$$\phi(s) = -k \cdot |\pi_s - \pi_{\text{goal}}|$$</p>
<p>$k$ is a scaling hyperparameter (set to 10 in my code) used to control the magnitude of the potential, which in turn amplifies the reward signal. A state closer to the goal has a higher potential (a less negative value). The policy-invariant shaping reward, $F$, is then derived from the change in this potential (setting discount factor $\gamma=1$ for the shaping term):</p>
<p>$$F(s, s&rsquo;) = \phi(s&rsquo;) - \phi(s)$$</p>
<p>By substituting the definition of $\phi(s)$, this yields the <code>reward_for_progress</code> term used in the code:</p>
<p>$$F = k \cdot \left( |\pi_t - \pi_{\text{goal}}| - |\pi_{t+1}^{\text{expected}} - \pi_{\text{goal}}| \right)$$</p>
<p>This shaping reward, $F$, acts as the primary guiding signal. However, the overall objective also includes a preference for policy stability. This is captured by a <strong>policy volatility penalty</strong>, $-\alpha (\Delta i_t)^2$. Note that this penalty is treated separately and is not part of the potential function. The technical reason is that a potential function $\psi(s)$ can, by definition, only be a function of the <strong>state</strong> ($s$). The interest rate change, $\Delta i_t$, is a direct consequence of the agent&rsquo;s action, not a feature of the state itself. Therefore, it mathematically cannot be included in $\psi(s)$.</p>
<p>This penalty, along with the terminal rewards (<code>+/- 100</code>), constitutes the &ldquo;original&rdquo; reward, $R_{original}$, which defines the fundamental constraints and objectives of the problem. The final reward given to the agent, $R_{total}$, is the sum of this true objective and the helpful shaping hint:</p>
<div>
$$R_{\text{total}} = \underbrace{ \left[ k \cdot (\text{error}_t - \text{error}_{t+1}^{\text{expected}}) \right] }_{F: \text{The Policy-Invariant Hint}} + \underbrace{ \left( -\alpha (\Delta i_t)^2 \pm 100_{\text{if terminal}} \right) }_{R_{original}: \text{The True Objective}}$$
<div>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">gym</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">gym</span> <span class="kn">import</span> <span class="n">spaces</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">gym.utils</span> <span class="kn">import</span> <span class="n">seeding</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># ==============================================================================</span>
</span></span><span class="line"><span class="cl"><span class="c1"># PART 1: THE ECONOMIC ENVIRONMENT</span>
</span></span><span class="line"><span class="cl"><span class="c1"># ==============================================================================</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">EconomyEnv</span><span class="p">(</span><span class="n">gym</span><span class="o">.</span><span class="n">Env</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    A custom Gym environment simulating a simple economy based on the Phillips Curve.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">metadata</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;render.modes&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;human&#39;</span><span class="p">]}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                 <span class="n">inflation_persistence</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                 <span class="n">rate_sensitivity</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                 <span class="n">policy_change_cost</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                 <span class="n">max_episode_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                 <span class="n">potential_scaling_factor</span><span class="o">=</span><span class="mi">10</span>
</span></span><span class="line"><span class="cl">                <span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># --- Economic Model Parameters ---</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">inflation_persistence</span>    <span class="c1"># How much past inflation affects current inflation</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">rate_sensitivity</span>      <span class="c1"># How much interest rates affect inflation</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">policy_change_cost</span>      <span class="c1"># Penalty for volatile policy changes</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">kappa</span> <span class="o">=</span> <span class="n">potential_scaling_factor</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># --- Environment Configuration ---</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">shock_std_dev</span> <span class="o">=</span> <span class="mf">0.5</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">goal_inflation</span> <span class="o">=</span> <span class="mf">2.0</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_max_episode_steps</span> <span class="o">=</span> <span class="n">max_episode_steps</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># --- State Space Boundaries ---</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">min_inflation</span> <span class="o">=</span> <span class="o">-</span><span class="mf">2.0</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">max_inflation</span> <span class="o">=</span> <span class="mf">8.0</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">min_interest_rate</span> <span class="o">=</span> <span class="mf">0.0</span>  <span class="c1"># Enforce the Zero Lower Bound (ZLB)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">max_interest_rate</span> <span class="o">=</span> <span class="mf">10.0</span> <span class="c1"># </span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># --- Gym Setup ---</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">state</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_current_step</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Define observation space (a continuous range of values)</span>
</span></span><span class="line"><span class="cl">        <span class="n">obs_low</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">min_inflation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_interest_rate</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">obs_high</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">max_inflation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_interest_rate</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">observation_space</span> <span class="o">=</span> <span class="n">spaces</span><span class="o">.</span><span class="n">Box</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="n">obs_low</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">obs_high</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Define action space (a discrete set of 5 actions)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">action_space</span> <span class="o">=</span> <span class="n">spaces</span><span class="o">.</span><span class="n">Discrete</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_action_map</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="mi">0</span><span class="p">:</span> <span class="o">-</span><span class="mf">50.0</span><span class="p">,</span> <span class="c1"># Large Cut</span>
</span></span><span class="line"><span class="cl">            <span class="mi">1</span><span class="p">:</span> <span class="o">-</span><span class="mf">25.0</span><span class="p">,</span> <span class="c1"># Standard Cut</span>
</span></span><span class="line"><span class="cl">            <span class="mi">2</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>   <span class="c1"># Hold</span>
</span></span><span class="line"><span class="cl">            <span class="mi">3</span><span class="p">:</span> <span class="mf">25.0</span><span class="p">,</span>  <span class="c1"># Standard Hike</span>
</span></span><span class="line"><span class="cl">            <span class="mi">4</span><span class="p">:</span> <span class="mf">50.0</span>   <span class="c1"># Large Hike</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">seed</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">seed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">np_random</span><span class="p">,</span> <span class="n">seed</span> <span class="o">=</span> <span class="n">seeding</span><span class="o">.</span><span class="n">np_random</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="p">[</span><span class="n">seed</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;Execute one time step within the environment.&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="n">current_inflation</span><span class="p">,</span> <span class="n">current_interest_rate</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># --- Action ---</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Map the discrete action (0-4) to a change in basis points</span>
</span></span><span class="line"><span class="cl">        <span class="n">bps_change</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_action_map</span><span class="p">[</span><span class="n">action</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">new_interest_rate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">current_interest_rate</span> <span class="o">+</span> <span class="p">(</span><span class="n">bps_change</span> <span class="o">/</span> <span class="mf">100.0</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">min_interest_rate</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">max_interest_rate</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># --- Reward Calculation (via Reward Shaping) ---</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># This &#34;dense&#34; reward function guides the agent at every step.</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 1. Calculate the deviation from the inflation target in the current state.</span>
</span></span><span class="line"><span class="cl">        <span class="n">current_deviation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">current_inflation</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">goal_inflation</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 2. Calculate the expected deviation in the next state.</span>
</span></span><span class="line"><span class="cl">        <span class="n">expected_next_inflation</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">*</span> <span class="n">current_inflation</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">*</span> <span class="n">new_interest_rate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">next_deviation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">expected_next_inflation</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">goal_inflation</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 3. The primary reward is the *improvement* in deviation. A positive</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#    reward is given for getting closer to the target.</span>
</span></span><span class="line"><span class="cl">        <span class="n">reward_for_progress</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kappa</span> <span class="o">*</span> <span class="p">(</span><span class="n">current_deviation</span> <span class="o">-</span> <span class="n">next_deviation</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 4. A small penalty is applied for making large, volatile policy changes.</span>
</span></span><span class="line"><span class="cl">        <span class="n">policy_penalty</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">bps_change</span> <span class="o">/</span> <span class="mf">100.0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
</span></span><span class="line"><span class="cl">        <span class="n">reward</span> <span class="o">=</span> <span class="n">reward_for_progress</span> <span class="o">-</span> <span class="n">policy_penalty</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># --- State Transition ---</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># The actual next state is the expected state plus a random economic shock.</span>
</span></span><span class="line"><span class="cl">        <span class="n">economic_shock</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">np_random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shock_std_dev</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">new_inflation</span> <span class="o">=</span> <span class="n">expected_next_inflation</span> <span class="o">+</span> <span class="n">economic_shock</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_current_step</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># --- Terminal Conditions ---</span>
</span></span><span class="line"><span class="cl">        <span class="n">is_unstable</span> <span class="o">=</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_inflation</span> <span class="o">&lt;=</span> <span class="n">new_inflation</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_inflation</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">is_over_time</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_current_step</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_episode_steps</span>
</span></span><span class="line"><span class="cl">        <span class="n">is_successful</span> <span class="o">=</span> <span class="nb">bool</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">new_inflation</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">goal_inflation</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">0.2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">done</span> <span class="o">=</span> <span class="n">is_unstable</span> <span class="ow">or</span> <span class="n">is_over_time</span> <span class="ow">or</span> <span class="n">is_successful</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Add large terminal rewards/penalties to strongly guide final outcomes.</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">is_unstable</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">reward</span> <span class="o">-=</span> <span class="mf">100.0</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">is_successful</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">reward</span> <span class="o">+=</span> <span class="mf">100.0</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Update the state and return results</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
</span></span><span class="line"><span class="cl">            <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">new_inflation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_inflation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_inflation</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">new_interest_rate</span>
</span></span><span class="line"><span class="cl">        <span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;Reset the environment to a random initial state.&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_current_step</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Start each episode in a random valid state to ensure broad learning</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span>
</span></span></code></pre></div><h3 id="the-solution-q-learning">The Solution: Q-Learning<a hidden class="anchor" aria-hidden="true" href="#the-solution-q-learning">#</a></h3>
<p>To solve this problem, I use the Q-learning algorithm. The goal is to learn a &ldquo;quality&rdquo; function, or <strong>Q-function</strong>, denoted as $Q(s, a)$. This function represents the agent&rsquo;s estimate of the total discounted future reward it can expect to receive by taking action $a$ from state $s$. Because our state space is discretized, these values are stored in a multi-dimensional array—in this case, a 3D table (inflation bin, interest rate bin, action)—called the Q-table.</p>
<p>To learn these Q-values, the agent must systematically explore its environment. It follows an <strong>epsilon-greedy ($\epsilon$-greedy) policy</strong> to balance two competing needs:</p>
<ul>
<li><strong>Exploration:</strong> With a small probability, $\epsilon$, the agent takes a completely random action. This is critical for discovering new, potentially better strategies.</li>
<li><strong>Exploitation:</strong> With probability $1-\epsilon$, the agent chooses the action with the highest known Q-value for its current state, thereby exploiting its accumulated knowledge.</li>
</ul>
<p>Initially, $\epsilon$ is set to 1.0 (100% exploration), and it is gradually decayed over the course of training, causing the agent to increasingly rely on the optimal policy it has learned.</p>
<p>After taking an action and observing the immediate reward, $R_{t+1}$, and the resulting state, $s&rsquo;$, the agent updates its Q-table. This update is performed using the temporal-difference rule derived from the Bellman equation:</p>
<p>$$ Q_{new}(s, a) = Q_{old}(s, a) + \alpha \left[ R_{t+1} + \gamma \max_{a&rsquo;} Q(s&rsquo;, a&rsquo;) - Q_{old}(s, a) \right] $$</p>
<p>This equation nudges the old Q-value toward a new, more accurate estimate:</p>
<ul>
<li><strong>$\alpha$ (Alpha):</strong> The <strong>Learning Rate</strong>. A hyperparameter that controls how much new information overwrites old information.</li>
<li><strong>$\gamma$ (Gamma):</strong> The <strong>Discount Factor</strong>. It determines the present value of future rewards; a value close to 1 means the agent is patient and values long-term outcomes.</li>
<li><strong>$R_{t+1} + \gamma \max_{a&rsquo;} Q(s&rsquo;, a&rsquo;)$</strong>: This is the <strong>TD (Temporal-Difference) Target</strong>. It represents our new, improved estimate for the value of being in state $s$ and taking action $a$. It is calculated from the immediate reward we just received ($R_{t+1}$) plus the discounted value of the best possible action from the next state ($s&rsquo;$).</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># ==============================================================================</span>
</span></span><span class="line"><span class="cl"><span class="c1"># PART 2: THE Q-LEARNING AGENT </span>
</span></span><span class="line"><span class="cl"><span class="c1"># ==============================================================================</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">QLearningAgent</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;A standard agent that learns via the Q-learning algorithm.&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_bins</span><span class="p">,</span> <span class="n">num_actions</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">num_actions</span> <span class="o">=</span> <span class="n">num_actions</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>      <span class="c1"># Learning rate: How much to update Q-values</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>      <span class="c1"># Discount factor: Importance of future rewards</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>    <span class="c1"># Exploration rate: Probability of taking a random action</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">state_bins</span> <span class="o">+</span> <span class="p">(</span><span class="n">num_actions</span><span class="p">,))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">choose_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_tuple</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;Chooses an action using an epsilon-greedy policy.&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Exploration: Take a random action</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_actions</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Exploitation: Choose the action with the highest Q-value</span>
</span></span><span class="line"><span class="cl">            <span class="n">q_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">state_tuple</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">q_values</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">update_q_table</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;Updates the Q-value for a state-action pair using the Bellman equation.&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="n">old_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">next_max</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">next_state</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># The core of Q-learning</span>
</span></span><span class="line"><span class="cl">        <span class="n">new_value</span> <span class="o">=</span> <span class="n">old_value</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">next_max</span> <span class="o">-</span> <span class="n">old_value</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_value</span>
</span></span></code></pre></div><h3 id="implementation-discretization-and-training">Implementation: Discretization and Training<a hidden class="anchor" aria-hidden="true" href="#implementation-discretization-and-training">#</a></h3>
<p><strong>State Space Discretization:</strong> The economic environment is continuous, but a Q-table requires a finite number of states. I solve this by discretizing the state space, carving the continuous ranges of inflation and interest rates into a finite grid. An observed continuous state like <code>(inflation=2.17%, interest_rate=3.58%)</code> is then mapped to a specific cell in this grid (e.g., <code>[inflation_bin=21, interest_rate_bin=18]</code>). This grid forms the discrete states $s$ used to index the Q-table.</p>
<p><strong>Training Loop:</strong> The agent is trained over millions of episodes to populate the Q-table. In each episode, the agent&rsquo;s exploration rate, $\epsilon$, is gradually decayed, shifting its behavior from random exploration to exploiting the optimal policy it has learned. The final output is the converged Q-table, which acts as the agent&rsquo;s complete policy map.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># ==============================================================================</span>
</span></span><span class="line"><span class="cl"><span class="c1"># PART 3: TRAINING AND VISUALIZATION</span>
</span></span><span class="line"><span class="cl"><span class="c1"># ==============================================================================</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">discretize_state</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">bins</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;Converts a continuous state vector into a discrete tuple for Q-table indexing.&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">inflation_val</span><span class="p">,</span> <span class="n">interest_rate_val</span> <span class="o">=</span> <span class="n">state</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Find which bin each continuous value falls into</span>
</span></span><span class="line"><span class="cl">    <span class="n">inflation_bin</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">digitize</span><span class="p">(</span><span class="n">inflation_val</span><span class="p">,</span> <span class="n">bins</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">-</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    <span class="n">interest_rate_bin</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">digitize</span><span class="p">(</span><span class="n">interest_rate_val</span><span class="p">,</span> <span class="n">bins</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">-</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Clip values to ensure they are valid indices for the Q-table</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">inflation_bin</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">bins</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">-</span> <span class="mi">2</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">interest_rate_bin</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">bins</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># --- Hyperparameters ---</span>
</span></span><span class="line"><span class="cl">    <span class="n">NUM_EPISODES</span> <span class="o">=</span> <span class="mi">3_000_000</span>
</span></span><span class="line"><span class="cl">    <span class="n">NUM_INFLATION_BINS</span> <span class="o">=</span> <span class="mi">30</span>
</span></span><span class="line"><span class="cl">    <span class="n">NUM_INTEREST_BINS</span> <span class="o">=</span> <span class="mi">40</span>
</span></span><span class="line"><span class="cl">    <span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.1</span>
</span></span><span class="line"><span class="cl">    <span class="n">DISCOUNT_FACTOR</span> <span class="o">=</span> <span class="mf">0.99</span>
</span></span><span class="line"><span class="cl">    <span class="n">EPSILON_START</span> <span class="o">=</span> <span class="mf">1.0</span>
</span></span><span class="line"><span class="cl">    <span class="n">EPSILON_END</span> <span class="o">=</span> <span class="mf">0.01</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Epsilon decays over the first 80% of episodes</span>
</span></span><span class="line"><span class="cl">    <span class="n">EPSILON_DECAY_RATE</span> <span class="o">=</span> <span class="p">(</span><span class="n">EPSILON_START</span> <span class="o">-</span> <span class="n">EPSILON_END</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">NUM_EPISODES</span> <span class="o">*</span> <span class="mf">0.8</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># --- Initialization ---</span>
</span></span><span class="line"><span class="cl">    <span class="n">env</span> <span class="o">=</span> <span class="n">EconomyEnv</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">state_discretization_bins</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">        <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">min_inflation</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">max_inflation</span><span class="p">,</span> <span class="n">NUM_INFLATION_BINS</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">min_interest_rate</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">max_interest_rate</span><span class="p">,</span> <span class="n">NUM_INTEREST_BINS</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">q_table_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">NUM_INFLATION_BINS</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">NUM_INTEREST_BINS</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">agent</span> <span class="o">=</span> <span class="n">QLearningAgent</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">state_bins</span><span class="o">=</span><span class="n">q_table_shape</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_actions</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">alpha</span><span class="o">=</span><span class="n">LEARNING_RATE</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">gamma</span><span class="o">=</span><span class="n">DISCOUNT_FACTOR</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">epsilon</span><span class="o">=</span><span class="n">EPSILON_START</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">rewards_per_episode</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;--- Starting Training ---&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># --- Training Loop ---</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">NUM_EPISODES</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">state_continuous</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">state_discrete</span> <span class="o">=</span> <span class="n">discretize_state</span><span class="p">(</span><span class="n">state_continuous</span><span class="p">,</span> <span class="n">state_discretization_bins</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl">        <span class="n">total_episode_reward</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">choose_action</span><span class="p">(</span><span class="n">state_discrete</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">next_state_continuous</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">next_state_discrete</span> <span class="o">=</span> <span class="n">discretize_state</span><span class="p">(</span><span class="n">next_state_continuous</span><span class="p">,</span> <span class="n">state_discretization_bins</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">agent</span><span class="o">.</span><span class="n">update_q_table</span><span class="p">(</span><span class="n">state_discrete</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state_discrete</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">state_discrete</span> <span class="o">=</span> <span class="n">next_state_discrete</span>
</span></span><span class="line"><span class="cl">            <span class="n">total_episode_reward</span> <span class="o">+=</span> <span class="n">reward</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Decay epsilon after each episode</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">agent</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">&gt;</span> <span class="n">EPSILON_END</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">agent</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">-=</span> <span class="n">EPSILON_DECAY_RATE</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">rewards_per_episode</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_episode_reward</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Print progress update periodically</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">episode</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">50000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">avg_reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_per_episode</span><span class="p">[</span><span class="o">-</span><span class="mi">5000</span><span class="p">:])</span>
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Episode: </span><span class="si">{</span><span class="n">episode</span> <span class="o">+</span> <span class="mi">1</span><span class="si">:</span><span class="s2">&gt;7</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">NUM_EPISODES</span><span class="si">}</span><span class="s2"> | Avg Reward (last 5k): </span><span class="si">{</span><span class="n">avg_reward</span><span class="si">:</span><span class="s2">8.2f</span><span class="si">}</span><span class="s2"> | Epsilon: </span><span class="si">{</span><span class="n">agent</span><span class="o">.</span><span class="n">epsilon</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;--- Training Finished ---&#34;</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>--- Starting Training ---
Episode:   50000/3000000 | Avg Reward (last 5k):   -58.46 | Epsilon: 0.9794
Episode:  100000/3000000 | Avg Reward (last 5k):   -55.27 | Epsilon: 0.9587
Episode:  150000/3000000 | Avg Reward (last 5k):   -53.38 | Epsilon: 0.9381
Episode:  200000/3000000 | Avg Reward (last 5k):   -55.71 | Epsilon: 0.9175
Episode:  250000/3000000 | Avg Reward (last 5k):   -54.15 | Epsilon: 0.8969
Episode:  300000/3000000 | Avg Reward (last 5k):   -54.91 | Epsilon: 0.8762
Episode:  350000/3000000 | Avg Reward (last 5k):   -54.84 | Epsilon: 0.8556
Episode:  400000/3000000 | Avg Reward (last 5k):   -54.10 | Epsilon: 0.8350
Episode:  450000/3000000 | Avg Reward (last 5k):   -53.69 | Epsilon: 0.8144
Episode:  500000/3000000 | Avg Reward (last 5k):   -53.29 | Epsilon: 0.7937
Episode:  550000/3000000 | Avg Reward (last 5k):   -51.78 | Epsilon: 0.7731
Episode:  600000/3000000 | Avg Reward (last 5k):   -48.55 | Epsilon: 0.7525
Episode:  650000/3000000 | Avg Reward (last 5k):   -50.17 | Epsilon: 0.7319
Episode:  700000/3000000 | Avg Reward (last 5k):   -47.55 | Epsilon: 0.7112
Episode:  750000/3000000 | Avg Reward (last 5k):   -50.07 | Epsilon: 0.6906
Episode:  800000/3000000 | Avg Reward (last 5k):   -45.87 | Epsilon: 0.6700
Episode:  850000/3000000 | Avg Reward (last 5k):   -46.32 | Epsilon: 0.6494
Episode:  900000/3000000 | Avg Reward (last 5k):   -41.20 | Epsilon: 0.6287
Episode:  950000/3000000 | Avg Reward (last 5k):   -42.94 | Epsilon: 0.6081
Episode: 1000000/3000000 | Avg Reward (last 5k):   -43.61 | Epsilon: 0.5875
Episode: 1050000/3000000 | Avg Reward (last 5k):   -45.70 | Epsilon: 0.5669
Episode: 1100000/3000000 | Avg Reward (last 5k):   -40.52 | Epsilon: 0.5462
Episode: 1150000/3000000 | Avg Reward (last 5k):   -39.68 | Epsilon: 0.5256
Episode: 1200000/3000000 | Avg Reward (last 5k):   -40.72 | Epsilon: 0.5050
Episode: 1250000/3000000 | Avg Reward (last 5k):   -38.01 | Epsilon: 0.4844
Episode: 1300000/3000000 | Avg Reward (last 5k):   -35.41 | Epsilon: 0.4637
Episode: 1350000/3000000 | Avg Reward (last 5k):   -31.66 | Epsilon: 0.4431
Episode: 1400000/3000000 | Avg Reward (last 5k):   -35.64 | Epsilon: 0.4225
Episode: 1450000/3000000 | Avg Reward (last 5k):   -30.22 | Epsilon: 0.4019
Episode: 1500000/3000000 | Avg Reward (last 5k):   -32.97 | Epsilon: 0.3812
Episode: 1550000/3000000 | Avg Reward (last 5k):   -32.01 | Epsilon: 0.3606
Episode: 1600000/3000000 | Avg Reward (last 5k):   -32.68 | Epsilon: 0.3400
Episode: 1650000/3000000 | Avg Reward (last 5k):   -30.51 | Epsilon: 0.3194
Episode: 1700000/3000000 | Avg Reward (last 5k):   -29.57 | Epsilon: 0.2987
Episode: 1750000/3000000 | Avg Reward (last 5k):   -25.22 | Epsilon: 0.2781
Episode: 1800000/3000000 | Avg Reward (last 5k):   -27.00 | Epsilon: 0.2575
Episode: 1850000/3000000 | Avg Reward (last 5k):   -25.29 | Epsilon: 0.2369
Episode: 1900000/3000000 | Avg Reward (last 5k):   -23.25 | Epsilon: 0.2162
Episode: 1950000/3000000 | Avg Reward (last 5k):   -22.05 | Epsilon: 0.1956
Episode: 2000000/3000000 | Avg Reward (last 5k):   -28.22 | Epsilon: 0.1750
Episode: 2050000/3000000 | Avg Reward (last 5k):   -19.97 | Epsilon: 0.1544
Episode: 2100000/3000000 | Avg Reward (last 5k):   -22.47 | Epsilon: 0.1337
Episode: 2150000/3000000 | Avg Reward (last 5k):   -21.62 | Epsilon: 0.1131
Episode: 2200000/3000000 | Avg Reward (last 5k):   -18.51 | Epsilon: 0.0925
Episode: 2250000/3000000 | Avg Reward (last 5k):   -17.62 | Epsilon: 0.0719
Episode: 2300000/3000000 | Avg Reward (last 5k):   -14.52 | Epsilon: 0.0512
Episode: 2350000/3000000 | Avg Reward (last 5k):   -22.56 | Epsilon: 0.0306
Episode: 2400000/3000000 | Avg Reward (last 5k):   -15.22 | Epsilon: 0.0100
Episode: 2450000/3000000 | Avg Reward (last 5k):   -16.36 | Epsilon: 0.0100
Episode: 2500000/3000000 | Avg Reward (last 5k):   -21.82 | Epsilon: 0.0100
Episode: 2550000/3000000 | Avg Reward (last 5k):   -18.83 | Epsilon: 0.0100
Episode: 2600000/3000000 | Avg Reward (last 5k):   -17.20 | Epsilon: 0.0100
Episode: 2650000/3000000 | Avg Reward (last 5k):   -14.62 | Epsilon: 0.0100
Episode: 2700000/3000000 | Avg Reward (last 5k):   -12.12 | Epsilon: 0.0100
Episode: 2750000/3000000 | Avg Reward (last 5k):   -14.64 | Epsilon: 0.0100
Episode: 2800000/3000000 | Avg Reward (last 5k):   -13.53 | Epsilon: 0.0100
Episode: 2850000/3000000 | Avg Reward (last 5k):   -10.35 | Epsilon: 0.0100
Episode: 2900000/3000000 | Avg Reward (last 5k):   -13.32 | Epsilon: 0.0100
Episode: 2950000/3000000 | Avg Reward (last 5k):   -19.52 | Epsilon: 0.0100
Episode: 3000000/3000000 | Avg Reward (last 5k):   -14.81 | Epsilon: 0.0100
--- Training Finished ---
</code></pre>
<h3 id="extracting-the-policy-from-the-q-function">Extracting the Policy from the Q-Function<a hidden class="anchor" aria-hidden="true" href="#extracting-the-policy-from-the-q-function">#</a></h3>
<p>After training, the Q-table contains the agent&rsquo;s learned estimates for the value of each state-action pair, $Q(s, a)$. The goal is to derive the optimal policy, $\pi^*(s)$, which specifies the single best action to take in any given state $s$.</p>
<p>This is achieved by applying the <code>argmax</code> operator. The policy function is defined as the action $a$ that maximizes the Q-function for a given state $s$:</p>
<div>
$$
\pi^*(s) = \operatorname*{argmax}_a Q(s, a)
$$
<div>
<p>Where:</p>
<ul>
<li>$\pi^*(s)$ is the optimal action to take in state $s$.</li>
<li>$\operatorname*{argmax}_a$ is the operator that returns the action $a$ which maximizes the function.</li>
<li>$Q(s, a)$ is the learned value for each state-action pair.</li>
<li>The operation is performed over the set of all possible actions, <span>$a \in A ={0,1,2,3,4}$<span>.</li>
</ul>
<p>This operation is applied to every state in the grid to produce the final policy map.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># --- Visualization ---</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># 1. Plot Rewards Over Time</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Use a larger window for smoothing due to more episodes</span>
</span></span><span class="line"><span class="cl">    <span class="n">moving_avg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">convolve</span><span class="p">(</span><span class="n">rewards_per_episode</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5000</span><span class="p">)</span><span class="o">/</span><span class="mi">5000</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;valid&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">moving_avg</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;royalblue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;5000-Episode Moving Average&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Agent&#39;s Learning Progress Over Time&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&#34;Episode&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&#34;Total Reward&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 2. Plot the Learned Policy</span>
</span></span><span class="line"><span class="cl">    <span class="n">policy_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">q_table</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">action_labels</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">change</span><span class="si">:</span><span class="s2">+g</span><span class="si">}</span><span class="s2"> bps&#34;</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">change</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">_action_map</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">im</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">policy_matrix</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">origin</span><span class="o">=</span><span class="s1">&#39;lower&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">extent</span><span class="o">=</span><span class="p">[</span><span class="n">env</span><span class="o">.</span><span class="n">min_inflation</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">max_inflation</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">min_interest_rate</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">max_interest_rate</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="n">cbar</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">ticks</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">action_labels</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
</span></span><span class="line"><span class="cl">    <span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">action_labels</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span>
</span></span><span class="line"><span class="cl">    <span class="n">cbar</span><span class="o">.</span><span class="n">set_label</span><span class="p">(</span><span class="s1">&#39;Optimal Action (Interest Rate Change)&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">270</span><span class="p">,</span> <span class="n">labelpad</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&#34;Current Inflation (%)&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&#34;Current Interest Rate (%)&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">min_inflation</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">max_inflation</span><span class="p">,</span> <span class="mi">11</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">min_interest_rate</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">max_interest_rate</span><span class="p">,</span> <span class="mi">11</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Learned Policy: Optimal Action for Each State&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><p><img alt="png" loading="lazy" src="output_7_0.png"></p>
<p><img alt="png" loading="lazy" src="output_7_1.png"></p>
<h3 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h3>
<p>The final policy map shows the agent learned a rational, though imperfect, strategy. The policy correctly reflects the expected inverse relationship: the agent generally raises rates in response to high inflation and cuts them when rates are high while inflation is low. It also learned avoid cuts at the Zero Lower Bound where they would be penalized.</p>
<p>However, the policy&rsquo;s visibly &ldquo;noisy&rdquo; texture reveals its limitations. The transition between the &ldquo;hike&rdquo; and &ldquo;cut&rdquo; regimes is a wide, stochastic gradient rather than a clean boundary. This is primarily a consequence of incomplete convergence; while the agent improved significantly, it had not yet learned to achieve consistently positive outcomes, indicating that a longer training duration or further hyperparameter tuning would be necessary. This core issue is compounded by factors inherent to the model&rsquo;s design: the policy&rsquo;s resolution is limited by state space discretization, its values are influenced by environmental stochasticity, and the underlying tabular Q-learning algorithm cannot generalize between states to create a smoother result.</p>
<h2 id="works-cited">Works Cited<a hidden class="anchor" aria-hidden="true" href="#works-cited">#</a></h2>
<p>Ng, A. Y., Harada, D., &amp; Russell, S. J. (1999). Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping. In <em>Proceedings of the Sixteenth International Conference on Machine Learning (ICML &lsquo;99)</em> (pp. 278-287).</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>

    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">Nima Nikopour</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>

</body>

</html>

