<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blog on Nima Nikopour</title>
    <link>http://localhost:1313/blog/</link>
    <description>Recent content in Blog on Nima Nikopour</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 29 Jun 2025 12:00:00 -0700</lastBuildDate>
    <atom:link href="http://localhost:1313/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Optimizing Monetary Policy with Q-Learning</title>
      <link>http://localhost:1313/blog/q-learning-monetary-policy/</link>
      <pubDate>Sun, 29 Jun 2025 12:00:00 -0700</pubDate>
      <guid>http://localhost:1313/blog/q-learning-monetary-policy/</guid>
      <description>&lt;p&gt;Reinforcement learning provides a framework for solving dynamic optimization problems. This article offers a direct example of applying Q-learning to train an agent that conducts monetary policy in a simplified economic environment. This project is broken down into three core coding sections. I begin by defining the &lt;code&gt;EconomyEnv&lt;/code&gt; class, which simulates the simplified economy and contains its fundamental rules, including the state variables (inflation, interest rate) and the agent&amp;rsquo;s action space. Next, I build the &lt;code&gt;QLearningAgent&lt;/code&gt; class, whose responsibility is to learn the optimal policy by updating its Q-table based on rewards. The final section brings these two together, managing the training process, hyperparameter settings, and the discretization of the continuous state space required for a tabular Q-learning approach.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
